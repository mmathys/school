% ------------------------------------------------------------------------------------------------ %
% GRENZWERTSÄTZE
% ------------------------------------------------------------------------------------------------ %


\section{Grenzwertsätze}


% ------------------------------------------------------------------------------------------------ %
% GESETZ DER GROSSEN ZAHLEN
% ------------------------------------------------------------------------------------------------ %


\subsection{Gesetz der grossen Zahlen}

\begin{theorem}[Schwaches GGZ]
Für eine Folge $X_1,X_2,\ldots$ von unkorrelierten Zufallsvariablen, die alle den Erwartungswert $\mu = \E[X_i]$ und die Varianz $\var[X_i] = \sigma^2$ haben, gilt
$$
\overline{X}_n := \frac{1}{n} \sum_{i=1}^n X_i
\quad\overset{n\rightarrow\infty}{\longrightarrow}\quad
\mu = \E[X_i].
$$
Das heisst
$$
\P\left[\lvert\overline{X}_n-\mu\rvert > \epsilon \right] \overset{n\rightarrow\infty}{\longrightarrow} 0
\quad \forall \epsilon > 0.
$$
\end{theorem}

\begin{theorem}[Starkes GGZ]
Für eine Folge $X_1,X_2,\ldots$ unabhängiger Zufallsvariablen, die alle den endlichen Erwartungswert $\mu = \E[X_i]$ haben, gilt
$$
\overline{X}_n := \frac{1}{n} \sum_{i=1}^n X_i
\quad\overset{n\rightarrow\infty}{\longrightarrow}\quad
\mu = \E[X_i].
\quad\text{P-fastsicher}
$$
Das heisst
$$
\P\left[\{\omega \in \Omega \mid \overline{X}_n(\omega) \overset{n\rightarrow\infty}{\longrightarrow} \mu\}\right] = 1.
$$
\end{theorem}


% ------------------------------------------------------------------------------------------------ %
% ZENTRALER GRENZWERTSATZ
% ------------------------------------------------------------------------------------------------ %


\subsection{Zentraler Grenzwertsatz}

\begin{theorem}[ZGS]
Sei $X_1,X_2,\ldots$ eine Folge von i.i.d. Zufallsvariablen mit $\mu = \E[X_i]$ und $\sigma^2 = \var[X_i]$, dann gilt für die Summe $S_n = \sum_{i=1}^n X_i$
$$
\lim_{n\rightarrow\infty} \P\left[ \frac{S_n-n\mu}{\sigma\sqrt{n}} \leq t \right] = \Phi(t) \quad \forall t \in \R
$$
wobei $\Phi$  die Verteilungsfunktion von $\mathcal{N}(0,1)$ ist.
\end{theorem}

\begin{note}
Die Summe $S_n$ hat Erwartungswert $\E[S_n] = n\mu$ und Varianz $\var[S_n] = n \sigma^2$. Die Grösse
$$
S_n^\ast := \frac{S_n-n\mu}{\sigma\sqrt{n}} = \frac{S_n - \E[S_n]}{\sqrt{\var[S_n]}}
$$
hat Erwartungswert $0$ und Varianz $1$. Für grosse $n$ gilt zudem:
$$\begin{array}{rcl}
\P[S_n^\ast \leq x] & \approx & \Phi(x) \\
S_n^\ast & \overset{\text{approx.}}{\sim} & \mathcal{N}(0,1) \\
S_n & \overset{\text{approx.}}{\sim} & \mathcal{N}(n\mu,n\sigma^2)
\end{array}$$
\end{note}


% ------------------------------------------------------------------------------------------------ %
% CHEBYSHEV UNGLEICHUNG
% ------------------------------------------------------------------------------------------------ %

\subsection{Chebyshev-Ungleichung}

Für eine Zufallsvariable $Y$ mit Erwartungswert $\mu_Y$ und Varianz $\sigma_Y^2$ und jedes $\epsilon > 0$ gilt
$$
\P[ \lvert Y - \mu_Y \rvert > \epsilon ] \leq \frac{\sigma_Y^2}{\epsilon^2}.
$$

% ------------------------------------------------------------------------------------------------ %
% MONTE CARLO
% ------------------------------------------------------------------------------------------------ %


\subsection{Monte Carlo Integration}

Das Integral
$$
I := \int_0^1 g(x) dx
$$
lässt sich als Erwartungswert auffassen, denn mit einer gleichverteilten Zufallsvariable $U \sim \mathcal{U}(0,1)$ folgt
$$
\E[g(U)] = \int_{-\infty}^\infty g(x) f_U(x) \d x = \int_0^1 g(x) \d x.
$$
Mit einer Folge von Zufallsvariablen $U_1,\ldots,U_n$, die unabhängig gleichverteilt $U_i \sim \mathcal{U}(0,1)$ sind, lässt sich das Integral approximieren: Nach dem schwachen Gesetz der grossen Zahlen gilt
$$
\overline{g(U_n)} = \frac{1}{n}\sum_{i=1}^n g(U_i) \overset{n\rightarrow\infty}{\longrightarrow} \E[g(U_1)] = I.
$$


% ------------------------------------------------------------------------------------------------ %